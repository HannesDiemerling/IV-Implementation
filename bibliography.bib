
@article{boucheron_theory_2005,
	title = {Theory of {Classification}: a {Survey} of {Some} {Recent} {Advances}},
	volume = {9},
	issn = {1292-8100, 1262-3318},
	shorttitle = {Theory of {Classification}},
	url = {https://www.cambridge.org/core/journals/esaim-probability-and-statistics/article/abs/theory-of-classification-a-survey-of-some-recent-advances/42A9912D17169A650AB06244820464BC},
	doi = {10.1051/ps:2005018},
	abstract = {The last few years have witnessed important new developments inthe theory and practice of pattern classification. We intend tosurvey some of the main new ideas that have led to theserecent results.},
	language = {en},
	urldate = {2025-02-01},
	journal = {ESAIM: Probability and Statistics},
	author = {Boucheron, Stéphane and Bousquet, Olivier and Lugosi, Gábor},
	month = nov,
	year = {2005},
	keywords = {Pattern recognition, statistical learning theory, concentration inequalities, empirical processes, model selection.},
	pages = {323--375},
}

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1053964/},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast łeq R łeq R{\textasciicircum}{\textbackslash}ast(2 –MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	number = {1},
	urldate = {2025-02-01},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	pages = {21--27},
}

@article{wee_comparison_2000,
	title = {Comparison of impression materials for direct multi-implant impressions},
	volume = {83},
	issn = {0022-3913},
	url = {https://www.sciencedirect.com/science/article/pii/S0022391300701363},
	doi = {10.1016/S0022-3913(00)70136-3},
	abstract = {Statement of problem. Given that meticulous implant prosthodontic procedures are recommended to obtain the best possible intraoral fit, impression materials that are suitable for use with a direct impression technique warrant further investigation. Purpose. This in vitro study compared the amount of torque required to rotate a square impression coping in an impression and evaluated the accuracy of solid implant casts fabricated from different impression materials. Material and methods. Two direct transfer implant impressions were made using 8 impression materials; the torque required to rotate an impression coping in the impressions was calculated. Ten direct transfer implant impressions were made from the master model and poured in a die stone (Resin Rock) for 3 of the 8 initial impression material groups. Linear distances between steel balls placed on each abutment replica were measured with a traveling microscope to determine distortion in the impression procedure for each group. Data were analyzed (P =.05) with ANOVA and Ryan-Einot-Gabriel-Welsch multiple range test for post hoc. Results. With a 1-way ANOVA, average torque values among the material groups differed significantly (P =.001). Polyether (medium consistency) was found to produce the highest overall torque values, followed by addition silicone (high consistency), and then polysulfide (medium consistency). Statistically significant difference was also found among the 3 material groups’ mean absolute cast error using a 1-way ANOVA (P =.0086). Implant casts made from polyether (medium) or addition silicone (high) impressions were significantly more accurate than casts made from polysulfide medium impressions. Conclusion. On the basis of the results of this study, the use of either polyether (medium) or addition silicone (high) impression is recommended for direct implant impressions. (J Prosthet Dent 2000;83:323-31.)},
	number = {3},
	urldate = {2025-02-01},
	journal = {The Journal of Prosthetic Dentistry},
	author = {Wee, Alvin G.},
	month = mar,
	year = {2000},
	pages = {323--331},
}

@article{weisberg_gender_2011,
	title = {Gender {Differences} in {Personality} across the {Ten} {Aspects} of the {Big} {Five}},
	volume = {2},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2011.00178/full},
	doi = {10.3389/fpsyg.2011.00178},
	abstract = {{\textless}p{\textgreater}This paper investigates gender differences in personality traits, both at the level of the Big Five and at the sublevel of two aspects within each Big Five domain. Replicating previous findings, women reported higher Big Five Extraversion, Agreeableness, and Neuroticism scores than men. However, more extensive gender differences were found at the level of the aspects, with significant gender differences appearing in both aspects of every Big Five trait. For Extraversion, Openness, and Conscientiousness, the gender differences were found to diverge at the aspect level, rendering them either small or undetectable at the Big Five level. These findings clarify the nature of gender differences in personality and highlight the utility of measuring personality at the aspect level.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-01},
	journal = {Frontiers in Psychology},
	author = {Weisberg, Yanna J. and DeYoung, Colin G. and Hirsh, Jacob B.},
	month = aug,
	year = {2011},
	keywords = {Big Five, gender differences, Personality},
}

@article{zhao_comparison_2020,
	title = {Comparison of clinical characteristics and outcomes of patients with coronavirus disease 2019 at different ages},
	volume = {12},
	issn = {1945-4589},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7346026/},
	doi = {10.18632/aging.103298},
	abstract = {Background: Information about the clinical characteristics and mortality of patients with coronavirus disease 2019 at different ages is limited., Results: The older group had more patients with dyspnea and fewer patients with fever and muscle pain. Older patients had more underlying diseases, secondary infection, myocardial injury, renal dysfunction, coagulation dysfunction, and immune dysfunction on admission. More older patients received immunoglobulin therapy and mechanical ventilation. The proportions of patients with multiple organ injuries, critically ill patients and death increased significantly with age. The older groups had higher cumulative death risk than the younger group. Hypertension, cerebrovascular disease, comorbidities, acute cardiac injury, shock and complications are independent predictors of death., Conclusions: The symptoms of the elderly patients were more atypical, with more comorbidities, secondary infection, organ injuries, immune dysfunction and a higher risk of critical illness. Older age was an important risk factor for mortality., Methods: 1000 patients diagnosed with coronavirus disease 2019 from January 1, 2020 to February 14, 2020 were enrolled. According to age, patients were divided into group 1 ({\textless}60 years old), group 2 (60-74 years old) and group 3 (≥75 years old). The clinical symptoms, first laboratory results, CT findings, organ injuries, disease severity and mortality were analyzed.},
	number = {11},
	urldate = {2025-02-01},
	journal = {Aging (Albany NY)},
	author = {Zhao, Mengmeng and Wang, Menglong and Zhang, Jishou and Gu, Jian and Zhang, Pingan and Xu, Yao and Ye, Jing and Wang, Zhen and Ye, Di and Pan, Wei and Shen, Bo and He, Hua and Liu, Mingxiao and Liu, Menglin and Luo, Zhen and Li, Dan and Liu, Jianfang and Wan, Jun},
	month = jun,
	year = {2020},
	pmid = {32499448},
	pmcid = {PMC7346026},
	pages = {10070--10086},
}

@article{counsell_reporting_2017,
	title = {Reporting practices and use of quantitative methods in {Canadian} journal articles in psychology},
	volume = {58},
	issn = {1878-7304},
	doi = {10.1037/cap0000074},
	abstract = {With recent focus on the state of research in psychology, it is essential to assess the nature of the statistical methods and analyses used and reported by psychological researchers. To that end, we investigated the prevalence of different statistical procedures and the nature of statistical reporting practices in recent articles from the 4 major Canadian psychology journals. The majority of authors evaluated their research hypotheses through the use of analysis of variance, t tests, and multiple regression. Multivariate approaches were less common. Null hypothesis significance testing remains a popular strategy, but the majority of authors reported a standardized or unstandardized effect size measure alongside their significance test results. Confidence intervals on effect sizes were infrequently employed. Many authors provided minimal details about their statistical analyses and less than a third of the articles presented on data complications such as missing data and violations of statistical assumptions. Strengths of and areas needing improvement for reporting quantitative results are highlighted. The article concludes with recommendations for how researchers and reviewers can improve comprehension and transparency in statistical reporting. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {2},
	journal = {Canadian Psychology / Psychologie canadienne},
	author = {Counsell, Alyssa and Harlow, Lisa. L.},
	year = {2017},
	keywords = {Psychology, Quantitative Methods, Scientific Communication, Statistics, Analysis of Variance, Confidence Limits (Statistics), Effect Size (Statistical), Multiple Regression, Statistical Significance, T Test},
	pages = {140--147},
}

@book{beins_research_2017,
	title = {Research {Methods} and {Statistics}},
	isbn = {9781108626910},
	abstract = {Research Methods and Statistics provides a seamless introduction to the subject, identifying various research areas and analyzing how one can approach them statistically. The text provides a solid empirical foundation for undergraduate psychology majors, and prepares the reader to think critically, and evaluate psychological research and claims they might hear in the news or popular press. The text can be used in all statistics, methods and experimental psychology courses.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Beins, Bernard C. and McCarthy, Maureen A.},
	month = nov,
	year = {2017},
	note = {Google-Books-ID: y8lBDwAAQBAJ},
	keywords = {Reference / Research, Language Arts \& Disciplines / Library \& Information Science / General, Social Science / Research},
}

@article{kim_classifiers_2018,
	title = {Classifiers as a model-free group comparison test},
	volume = {50},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-017-0880-z},
	doi = {10.3758/s13428-017-0880-z},
	abstract = {The conventional statistical methods to detect group differences assume correct model specification, including the origin of difference. Researchers should be able to identify a source of group differences and choose a corresponding method. In this paper, we propose a new approach of group comparison without model specification using classification algorithms in machine learning. In this approach, the classification accuracy is evaluated against a binomial distribution using Independent Validation. As an application example, we examined false-positive errors and statistical power of support vector machines to detect group differences in comparison to conventional statistical tests such as t test, Levene’s test, K-S test, Fisher’s z-transformation, and MANOVA. The SVMs detected group differences regardless of their origins (mean, variance, distribution shape, and covariance), and showed comparably consistent power across conditions. When a group difference originated from a single source, the statistical power of SVMs was lower than the most appropriate conventional test of the study condition; however, the power of SVMs increased when differences originated from multiple sources. Moreover, SVMs showed substantially improved performance with more variables than with fewer variables. Most importantly, SVMs were applicable to any types of data without sophisticated model specification. This study demonstrates a new application of classification algorithms as an alternative or complement to the conventional group comparison test. With the proposed approach, researchers can test two-sample data even when they are not certain which statistical test to use or when data violates the statistical assumptions of conventional methods.},
	language = {en},
	number = {1},
	urldate = {2025-02-01},
	journal = {Behavior Research Methods},
	author = {Kim, Bommae and Oertzen, Timo von},
	month = feb,
	year = {2018},
	keywords = {Group comparison, Classifiers, Support vector machine, K-fold cross validation, Independent validation},
	pages = {416--426},
}

@article{hoekstra_are_2012,
	title = {Are {Assumptions} of {Well}-{Known} {Statistical} {Techniques} {Checked}, and {Why} ({Not})?},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2012.00137/full},
	doi = {10.3389/fpsyg.2012.00137},
	abstract = {{\textless}p{\textgreater}A valid interpretation of most statistical techniques requires that one or more assumptions be met. In published articles, however, little information tends to be reported on whether the data satisfy the assumptions underlying the statistical techniques used. This could be due to self-selection: Only manuscripts with data fulfilling the assumptions are submitted. Another explanation could be that violations of assumptions are rarely checked for in the first place. We studied whether and how 30 researchers checked fictitious data for violations of assumptions in their own working environment. Participants were asked to analyze the data as they would their own data, for which often used and well-known techniques such as the {\textless}italic{\textgreater}t{\textless}/italic{\textgreater}-procedure, ANOVA and regression (or non-parametric alternatives) were required. It was found that the assumptions of the techniques were rarely checked, and that if they were, it was regularly by means of a statistical test. Interviews afterward revealed a general lack of knowledge about assumptions, the robustness of the techniques with regards to the assumptions, and how (or whether) assumptions should be checked. These data suggest that checking for violations of assumptions is not a well-considered choice, and that the use of statistics can be described as opportunistic.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-01},
	journal = {Frontiers in Psychology},
	author = {Hoekstra, Rink and Kiers, Henk and Johnson, Addie},
	month = may,
	year = {2012},
	keywords = {analyzing data, assumptions, homogeneity, normality, robustness},
}

@article{bay_detecting_2001,
	title = {Detecting {Group} {Differences}: {Mining} {Contrast} {Sets}},
	volume = {5},
	issn = {1573-756X},
	shorttitle = {Detecting {Group} {Differences}},
	url = {https://doi.org/10.1023/A:1011429418057},
	doi = {10.1023/A:1011429418057},
	abstract = {A fundamental task in data analysis is understanding the differences between several contrasting groups. These groups can represent different classes of objects, such as male or female students, or the same group over time, e.g. freshman students in 1993 through 1998. We present the problem of mining contrast sets: conjunctions of attributes and values that differ meaningfully in their distribution across groups. We provide a search algorithm for mining contrast sets with pruning rules that drastically reduce the computational complexity. Once the contrast sets are found, we post-process the results to present a subset that are surprising to the user given what we have already shown. We explicitly control the probability of Type I error (false positives) and guarantee a maximum error rate for the entire analysis by using Bonferroni corrections.},
	language = {en},
	number = {3},
	urldate = {2025-02-01},
	journal = {Data Mining and Knowledge Discovery},
	author = {Bay, Stephen D. and Pazzani, Michael J.},
	month = jul,
	year = {2001},
	keywords = {
                Artificial Intelligence
            , data mining, contrast sets, change detection, association rules},
	pages = {213--246},
}

@inproceedings{ho_random_1995,
	title = {Random decision forests},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/598994/},
	doi = {10.1109/ICDAR.1995.598994},
	abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.},
	urldate = {2025-02-01},
	booktitle = {Proceedings of 3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Ho, Tin Kam},
	month = aug,
	year = {1995},
	keywords = {Classification tree analysis, Decision trees, Training data, Optimization methods, Testing, Tin, Stochastic processes, Handwriting recognition, Hidden Markov models, Multilayer perceptrons},
	pages = {278--282 vol.1},
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	language = {en},
	number = {3},
	urldate = {2025-02-01},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	keywords = {
                Artificial Intelligence
            , pattern recognition, efficient learning algorithms, neural networks, radial basis function classifiers, polynomial classifiers},
	pages = {273--297},
}

@article{braun_independent_2023,
	title = {Independent {Validation} as a {Validation} {Method} for {Classification}},
	copyright = {Copyright (c) 2023 Tina Braun, Hannes Eckert, Timo von Oertzen},
	issn = {2699-8432},
	url = {https://qcmb.psychopen.eu/index.php/qcmb/article/view/12069},
	doi = {10.5964/qcmb.12069},
	abstract = {The use of classifiers provides an alternative to conventional statistical methods. This involves using the accuracy with which data is correctly assigned to a given group by the classifier to apply tests to compare the performance of classifiers. The conventional validation methods for determining the accuracy of classifiers have the disadvantage that the distribution of correct classifications does not follow any known distribution, and therefore, the application of statistical tests is problematic. Independent validation circumvents this problem and allows the use of binomial tests to assess the performance of classifiers. However, independent validation accuracy is subject to bias for small training datasets. The present study shows that a hyperbolic function can be used to estimate the loss in classifier accuracy for independent validation. This function is used to develop three new methods to estimate the classifier accuracy for small training sets more precisely. These methods are compared to two existing methods in a simulation study. The results indicate overall small errors in the estimation of classifier accuracy and indicate that independent validation can be used with small samples. A least square estimation approach seems best suited to estimate the classifier accuracy.},
	language = {en},
	urldate = {2025-02-01},
	journal = {Quantitative and Computational Methods in Behavioral Sciences},
	author = {Braun, Tina and Eckert, Hannes and Oertzen, Timo von},
	month = dec,
	year = {2023},
	keywords = {independent validation, classifiers, classifier accuracy, simulation},
	pages = {1--30},
}

@inproceedings{kohavi_study_1995,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'95},
	title = {A study of cross-validation and bootstrap for accuracy estimation and model selection},
	isbn = {9781558603639},
	abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
	urldate = {2025-02-01},
	booktitle = {Proceedings of the 14th international joint conference on {Artificial} intelligence - {Volume} 2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Kohavi, Ron},
	month = aug,
	year = {1995},
	pages = {1137--1143},
}

@article{sahiner_classifier_2008,
	series = {Advances in {Neural} {Networks} {Research}: {IJCNN} ’07},
	title = {Classifier performance estimation under the constraint of a finite sample size: {Resampling} schemes applied to neural network classifiers},
	volume = {21},
	issn = {0893-6080},
	shorttitle = {Classifier performance estimation under the constraint of a finite sample size},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608007002444},
	doi = {10.1016/j.neunet.2007.12.012},
	abstract = {In a practical classifier design problem the sample size is limited, and the available finite sample needs to be used both to design a classifier and to predict the classifier’s performance for the true population. Since a larger sample is more representative of the population, it is advantageous to design the classifier with all the available cases, and to use a resampling technique for performance prediction. We conducted a Monte Carlo simulation study to compare the ability of different resampling techniques in predicting the performance of a neural network (NN) classifier designed with the available sample. We used the area under the receiver operating characteristic curve as the performance index for the NN classifier. We investigated resampling techniques based on the cross-validation, the leave-one-out method, and three different types of bootstrapping, namely, the ordinary, .632, and .632+ bootstrap. Our results indicated that, under the study conditions, there can be a large difference in the accuracy of the prediction obtained from different resampling methods, especially when the feature space dimensionality is relatively large and the sample size is small. Although this investigation is performed under some specific conditions, it reveals important trends for the problem of classifier performance prediction under the constraint of a limited data set.},
	number = {2},
	urldate = {2025-02-01},
	journal = {Neural Networks},
	author = {Sahiner, Berkman and Chan, Heang-Ping and Hadjiiski, Lubomir},
	month = mar,
	year = {2008},
	keywords = {Performance estimation, Finite sample size, Resampling},
	pages = {476--483},
}

@article{santafe_dealing_2015,
	title = {Dealing with the evaluation of supervised classification algorithms},
	volume = {44},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-015-9433-y},
	doi = {10.1007/s10462-015-9433-y},
	abstract = {Performance assessment of a learning method related to its prediction ability on independent data is extremely important in supervised classification. This process provides the information to evaluate the quality of a classification model and to choose the most appropriate technique to solve the specific supervised classification problem at hand. This paper aims to review the most important aspects of the evaluation process of supervised classification algorithms. Thus the overall evaluation process is put in perspective to lead the reader to a deep understanding of it. Additionally, different recommendations about their use and limitations as well as a critical view of the reviewed methods are presented according to the specific characteristics of the supervised classification problem scenario.},
	language = {en},
	number = {4},
	urldate = {2025-02-01},
	journal = {Artificial Intelligence Review},
	author = {Santafe, Guzman and Inza, Iñaki and Lozano, Jose A.},
	month = dec,
	year = {2015},
	keywords = {
                Artificial Intelligence
            , Supervised classification, Classifier evaluation, Quality measures, Estimation methods, Classification algorithms comparison},
	pages = {467--508},
}

@article{lanka_supervised_2020,
	title = {Supervised machine learning for diagnostic classification from large-scale neuroimaging datasets},
	volume = {14},
	issn = {1931-7565},
	url = {https://doi.org/10.1007/s11682-019-00191-8},
	doi = {10.1007/s11682-019-00191-8},
	abstract = {There are growing concerns about the generalizability of machine learning classifiers in neuroimaging. In order to evaluate this aspect across relatively large heterogeneous populations, we investigated four disorders: Autism spectrum disorder (N = 988), Attention deficit hyperactivity disorder (N = 930), Post-traumatic stress disorder (N = 87) and Alzheimer’s disease (N = 132). We applied 18 different machine learning classifiers (based on diverse principles) wherein the training/validation and the hold-out test data belonged to samples with the same diagnosis but differing in either the age range or the acquisition site. Our results indicate that overfitting can be a huge problem in heterogeneous datasets, especially with fewer samples, leading to inflated measures of accuracy that fail to generalize well to the general clinical population. Further, different classifiers tended to perform well on different datasets. In order to address this, we propose a consensus-classifier by combining the predictive power of all 18 classifiers. The consensus-classifier was less sensitive to unmatched training/validation and holdout test data. Finally, we combined feature importance scores obtained from all classifiers to infer the discriminative ability of connectivity features. The functional connectivity patterns thus identified were robust to the classification algorithm used, age and acquisition site differences, and had diagnostic predictive ability in addition to univariate statistically significant group differences between the groups. A MATLAB toolbox called Machine Learning in NeuroImaging (MALINI), which implements all the 18 different classifiers along with the consensus classifier is available from Lanka et al. (2019) The toolbox can also be found at the following URL: https://github.com/pradlanka/malini.},
	language = {en},
	number = {6},
	urldate = {2025-02-01},
	journal = {Brain Imaging and Behavior},
	author = {Lanka, Pradyumna and Rangaprakash, D. and Dretsch, Michael N. and Katz, Jeffrey S. and Denney, Thomas S. and Deshpande, Gopikrishna},
	month = dec,
	year = {2020},
	keywords = {Resting-state functional MRI, Supervised machine learning, Diagnostic classification, Functional connectivity, Autism, ADHD, Alzheimer’s disease, PTSD},
	pages = {2378--2416},
}

@article{pesarin_permutation_2010,
	title = {The permutation testing approach: a review},
	volume = {70},
	copyright = {Copyright (c) 2010 Statistica},
	issn = {1973-2201},
	shorttitle = {The permutation testing approach},
	url = {https://rivista-statistica.unibo.it/article/view/3599},
	doi = {10.6092/issn.1973-2201/3599},
	abstract = {In recent years permutation testing methods have increased both in number of applications and in solving complex multivariate problems. A large number of testing problems may also be usefully and effectively solved by traditional parametric or rank-based nonparametric methods, although in relatively mild conditions their permutation counterparts are generally asymptotically as good as the best ones. Permutation tests are essentially of an exact nonparametric nature in a conditional context, where conditioning is on the pooled observed data as a set of sufficient statistics in the null hypothesis. Instead, the reference null distribution of most parametric tests is only known asymptotically. Thus, for most sample sizes of practical interest, the possible lack of efficiency of permutation solutions may be compensated by the lack of approximation of parametric counterparts. There are many complex multivariate problems (quite common in biostatistics, clinical trials, engineering, the environment, epidemiology, experimental data, industrial statistics, pharmacology, psychology, social sciences, etc.) which are difficult to solve outside the conditional framework and outside the nonparametric combination (NPC) method for dependent permutation tests. In this paper we review this method along with a number of applications in different experimental and observational situations (e.g. multi-sided alternatives, zero-inflated data and testing for a stochastic ordering) and we present properties specific to this methodology, such as: for a given number of subjects, when the number of variables diverges and the noncentrality of the combined test diverges accordingly, then the power of combination-based permutation tests converges to one.},
	language = {en},
	number = {4},
	urldate = {2025-02-01},
	journal = {Statistica},
	author = {Pesarin, Fortunato and Salmaso, Luigi},
	month = dec,
	year = {2010},
	pages = {481--509},
}

@article{fisher_frequency_1915,
	title = {Frequency {Distribution} of the {Values} of the {Correlation} {Coefficient} in {Samples} from an {Indefinitely} {Large} {Population}},
	volume = {10},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2331838?origin=crossref},
	doi = {10.2307/2331838},
	number = {4},
	urldate = {2025-02-01},
	journal = {Biometrika},
	author = {Fisher, R. A.},
	month = may,
	year = {1915},
	pages = {507},
}

@book{breiman_classification_2017,
	address = {New York},
	title = {Classification and {Regression} {Trees}},
	isbn = {9781315139470},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
	publisher = {Chapman and Hall/CRC},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, R. A. and Stone, Charles J.},
	month = oct,
	year = {2017},
	doi = {10.1201/9781315139470},
}

@incollection{rossi_statistical_2013,
	address = {Hoboken, NJ, US},
	title = {Statistical power analysis},
	isbn = {9780470890646 9781118282038 9781118282540 9781118286289},
	abstract = {In this chapter, the concept of statistical power and several closely related techniques and topics are presented. In recent years, there have been numerous calls for the increased use of statistical power analysis, along with effect sizes, confidence intervals, and meta-analysis, as alternatives to the traditional practice of null hypothesis significance testing and the use of p values as the principal method of establishing the evidence for an effect. There are three main study design factors that determine the power of a statistical test: sample size, effect size, and alpha level. It is well known that increasing any of these factors will increase statistical power. Unfortunately, in many situations it is often difficult to increase the power of a study by directly manipulating any of these factors. The alpha level at which a statistical test is conducted is typically too constrained by convention to permit much impact on power. Sample size is the easiest factor to change but is frequently constrained by available resources. Effect size is usually thought of as a property of the population and therefore not often considered by researchers as something that can be manipulated to increase power. Methods for calculating power are covered in this chapter, but the main emphasis is on examining how various aspects of study design can be affected so as to enhance the power of a study. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	booktitle = {Handbook of psychology: {Research} methods in psychology, {Vol}. 2, 2nd ed},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Rossi, Joseph S.},
	year = {2013},
	keywords = {Statistical Analysis, Statistical Power, Confidence Limits (Statistics), Effect Size (Statistical), Experimental Design, Experimenters, Meta Analysis, Null Hypothesis Testing, Sample Size},
	pages = {71--108},
}

@misc{kaggle-survey-2022,
    author = {Paul Mooney},
    title = {2022 Kaggle Machine Learning & Data Science Survey},
    year = {2022},
    howpublished = {\url{https://kaggle.com/competitions/kaggle-survey-2022}},
    note = {Kaggle}
}

@article{devroye_distribution-free_1979,
	title = {Distribution-free performance bounds for potential function rules},
	volume = {25},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1056087},
	doi = {10.1109/TIT.1979.1056087},
	abstract = {In the discrimination problem the random variableþeta, known to take values in1, {\textbackslash}cdots ,M, is estimated from the random vectorX. All that is known about the joint distribution of(X, þeta)is that which can be inferred from a sample(X\_1, þeta\_1), {\textbackslash}cdots ,(X\_n, þeta\_n)of sizendrawn from that distribution. A discrimination nde is any procedure which determines a decision{\textbackslash}hat þetaforþetafromXand(X\_1, þeta\_1) , {\textbackslash}cdots , (X\_n, þeta\_n). For rules which are determined by potential functions it is shown that the mean-square difference between the probability of error for the nde and its deleted estimate is bounded byA/ {\textbackslash}sqrtnwhereAis an explicitly given constant depending only onMand the potential function. TheO(n {\textasciicircum}-1/2)behavior is shown to be the best possible for one of the most commonly encountered rules of this type.},
	number = {5},
	urldate = {2025-02-01},
	journal = {IEEE Transactions on Information Theory},
	author = {Devroye, L. and Wagner, T.},
	month = sep,
	year = {1979},
	pages = {601--604},
}

@article{geisser_predictive_1975,
	title = {The {Predictive} {Sample} {Reuse} {Method} with {Applications}},
	volume = {70},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1975.10479865},
	doi = {10.1080/01621459.1975.10479865},
	language = {en},
	number = {350},
	urldate = {2025-02-01},
	journal = {Journal of the American Statistical Association},
	author = {Geisser, Seymour},
	month = jun,
	year = {1975},
	pages = {320--328},
}

@article{stone_cross-validatory_1974,
	title = {Cross-{Validatory} {Choice} and {Assessment} of {Statistical} {Predictions}},
	volume = {36},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/36/2/111/7027414},
	doi = {10.1111/j.2517-6161.1974.tb00994.x},
	abstract = {Summary
            A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
	language = {en},
	number = {2},
	urldate = {2025-02-01},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Stone, M.},
	month = jan,
	year = {1974},
	pages = {111--133},
}

@inproceedings{bouckaert_choosing_2003,
	address = {Washington, DC, USA},
	series = {{ICML}'03},
	title = {Choosing between two learning algorithms based on calibrated tests},
	isbn = {9781577351894},
	abstract = {Designing a hypothesis test to determine the best of two machine learning algorithms with only a small data set available is not a simple task. Many popular tests suffer from low power (5×2 cv [2]), or high Type I error (Weka's 10×10 cross validation [11]). Furthermore, many tests show a low level of replicability, so that tests performed by different scientists with the same pair of algorithms, the same data sets and the same hypothesis test still may present different results. We show that 5×2 cv, resampling and 10 fold cv suffer from low replicability.The main complication is due to the need to use the data multiple times. As a consequence, independence assumptions for most hypothesis tests are violated. In this paper, we pose the case that reuse of the same data causes the effective degrees of freedom to be much lower than theoretically expected. We show how to calibrate the effective degrees of freedom empirically for various tests. Some tests are not calibratable, indicating another flaw in the design. However the ones that are calibratable all show very similar behavior. Moreover, the Type I error of those tests is on the mark for a wide range of circumstances, while they show a power and replicability that is a considerably higher than currently popular hypothesis tests.},
	urldate = {2025-02-01},
	booktitle = {Proceedings of the {Twentieth} {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Bouckaert, Remco R.},
	month = aug,
	year = {2003},
	pages = {51--58},
}

@article{salzberg_comparing_1997,
	title = {On {Comparing} {Classifiers}: {Pitfalls} to {Avoid} and a {Recommended} {Approach}},
	volume = {1},
	issn = {1573-756X},
	shorttitle = {On {Comparing} {Classifiers}},
	url = {https://doi.org/10.1023/A:1009752403260},
	doi = {10.1023/A:1009752403260},
	abstract = {An important component of many data mining projects is finding a good classification algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classification and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classification, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies.},
	language = {en},
	number = {3},
	urldate = {2025-02-01},
	journal = {Data Mining and Knowledge Discovery},
	author = {Salzberg, Steven L.},
	month = sep,
	year = {1997},
	keywords = {
                Artificial Intelligence
            , classification, comparative studies, statistical methods},
	pages = {317--328},
}

@article{arlot_survey_2010,
	title = {A survey of cross-validation procedures for model selection},
	volume = {4},
	issn = {1935-7516},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full},
	doi = {10.1214/09-SS054},
	abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
	number = {none},
	urldate = {2025-02-01},
	journal = {Statistics Surveys},
	author = {Arlot, Sylvain and Celisse, Alain},
	month = jan,
	year = {2010},
	keywords = {62G05, 62G08, 62G09, cross-validation, leave-one-out, Model selection},
	pages = {40--79},
}

@article{tibshirani_bias_2009,
	title = {A bias correction for the minimum error rate in cross-validation},
	volume = {3},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-3/issue-2/A-bias-correction-for-the-minimum-error-rate-in-cross/10.1214/08-AOAS224.full},
	doi = {10.1214/08-AOAS224},
	abstract = {Tuning parameters in supervised learning problems are often estimated by cross-validation. The minimum value of the cross-validation error can be biased downward as an estimate of the test error at that same value of the tuning parameter. We propose a simple method for the estimation of this bias that uses information from the cross-validation process. As a result, it requires essentially no additional computation. We apply our bias estimate to a number of popular classifiers in various settings, and examine its performance.},
	number = {2},
	urldate = {2025-02-01},
	journal = {The Annals of Applied Statistics},
	author = {Tibshirani, Ryan J. and Tibshirani, Robert},
	month = jun,
	year = {2009},
	keywords = {cross-validation, optimism estimation, prediction error estimation},
	pages = {822--829},
}

@article{dietterich_approximate_1998,
	title = {Approximate {Statistical} {Tests} for {Comparing} {Supervised} {Classification} {Learning} {Algorithms}},
	volume = {10},
	issn = {1530-888X},
	doi = {10.1162/089976698300017197},
	abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 x 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 x 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 x 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
	language = {eng},
	number = {7},
	journal = {Neural Computation},
	author = {Dietterich, T. G.},
	month = sep,
	year = {1998},
	pmid = {9744903},
	pages = {1895--1923},
}


@book{r_development_core_team._r_2010,
	address = {[Vienna]},
	title = {R a language and environment for statistical computing: reference index},
	isbn = {9783900051075},
	shorttitle = {R a language and environment for statistical computing},
	language = {eng},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Development Core Team.}},
	year = {2010},
	note = {OCLC: 1120300286},
}

@techreport{independent_validation,
    author = {Kim, Bommae and von Oertzen, Timo},
    title = {Independent Validation remedies α Inflation in Classifier Accuracy Testing},
    institution = {Department of Psychology, University of Virginia, Charlottesville},
    year = {2017},
    type = {Technical Report},
    number = {Report number},
    address = {Timo von Oertzen, Department of Psychology, University of Virginia, 1023 Millmont Street, VA22903 Charlottesville.}
}

@incollection{fisher1970statistical,
  title={Statistical methods for research workers},
  author={Fisher, Ronald Aylmer},
  booktitle={Breakthroughs in statistics: Methodology and distribution},
  pages={66--70},
  year={1970},
  publisher={Springer}
}

@article{student1908probable,
  title={The probable error of a mean},
  author={Student},
  journal={Biometrika},
  pages={1--25},
  year={1908},
  publisher={JSTOR}
}

@article{pearson1900x,
  title={X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={50},
  number={302},
  pages={157--175},
  year={1900},
  publisher={Taylor \& Francis}
}

@misc{levene1960robust,
  title={Robust tests for equality of variances},
  author={Levene, H},
  year={1960},
  publisher={Contributions to Probability and Statistics: Essays in Honor of Harold~…}
}

@misc{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor},
  year={2009},
  publisher={Springer}
}

@article{diemerling2024implementing,
  title={Implementing machine learning techniques for continuous emotion prediction from uniformly segmented voice recordings},
  author={Diemerling, Hannes and Stresemann, Leonie and Braun, Tina and Von Oertzen, Timo},
  journal={Frontiers in Psychology},
  volume={15},
  pages={1300996},
  year={2024},
  publisher={Frontiers Media SA}
}

@article{hastings1970monte,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}

@article{metropolis1953equation,
  title={Equation of state calculations by fast computing machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The journal of chemical physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={American Institute of Physics}
}

@article{van_rossum_python_1995,
	title = {Python tutorial},
	url = {https://ir.cwi.nl/pub/5007},
	abstract = {Python is a simple, yet powerful programming language that bridges the gap between C and shell programming, and is thus ideally suited for ``throw-away programming'' and rapid prototyping. Its syntax is put together from constructs borrowed from a variety of other languages; most prominent are influences from ABC, C, Modula-3 and Icon. The Python interpreter is easily extended with new functions and data types implemented in C. Python is also suitable as an extension language for highly customizable C applications such as editors or window managers. Python is available for various operating systems, amongst which several flavors of UNIX, Amoeba, the Apple Macintosh O.S., and MS-DOS. This tutorial introduces the reader informally to the basic concepts and features of the Python language and system. It helps to have a Python interpreter handy for hands-on experience, but as the examples are self-contained, the tutorial can be read off-line as well. For a description of standard objects and modules, see the Python Library Reference manual. The Python Reference Manual gives a more formal definition of the language.},
	language = {en},
	urldate = {2025-02-13},
	author = {van Rossum, Guido},
	month = jan,
	year = {1995},
}

@misc{touvron_llama:_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971},
	keywords = {Computer Science - Computation and Language},
}

@article{picard_cross-validation_1984,
	title = {Cross-{Validation} of {Regression} {Models}},
	volume = {79},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478083},
	doi = {10.1080/01621459.1984.10478083},
	language = {en},
	number = {387},
	urldate = {2025-02-13},
	journal = {Journal of the American Statistical Association},
	author = {Picard, Richard R. and Cook, R. Dennis},
	month = sep,
	year = {1984},
	pages = {575--583},
}

@article{aeberhard1994comparative,
  title={Comparative analysis of statistical pattern recognition methods in high dimensional settings},
  author={Aeberhard, Stefan and Coomans, Danny and De Vel, Olivier},
  journal={Pattern Recognition},
  volume={27},
  number={8},
  pages={1065--1077},
  year={1994},
  publisher={Elsevier}
}
